1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
4:I[3063,["63","static/chunks/63-97ed399e744b3a55.js","194","static/chunks/app/convolutional-autoencoder/page-5d297ce251709cb8.js"],"Image"]
5:I[9665,[],"MetadataBoundary"]
7:I[9665,[],"OutletBoundary"]
a:I[4911,[],"AsyncMetadataOutlet"]
c:I[9665,[],"ViewportBoundary"]
e:I[6614,[],""]
:HL["/portfolio/_next/static/media/569ce4b8f30dc480-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/portfolio/_next/static/media/93f479601ee12b01-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/portfolio/_next/static/css/5d0a0b4953cccf79.css","style"]
0:{"P":null,"b":"57UcXMGoAR3rUwblzJLhQ","p":"/portfolio","c":["","convolutional-autoencoder"],"i":false,"f":[[["",{"children":["convolutional-autoencoder",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/portfolio/_next/static/css/5d0a0b4953cccf79.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__variable_062a20 __variable_f9491c antialiased","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["convolutional-autoencoder",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"flex flex-col m-10 gap-2 items-center","children":[["$","h1",null,{"className":"bg-clip-text bg-gradient-to-r from-blue-400 to-purple-400 text-transparent text-4xl font-sans font-bold text-center max-w-200","children":"Image Dimensionality Reduction Using Convolutional Autoencoder"}],["$","p",null,{"className":"border-1 w-full lg:w-200 mb-8"}],["$","div",null,{"className":"flex flex-col gap-10","children":[["$","div",null,{"className":"max-w-200","children":[["$","h2",null,{"className":"bg-clip-text bg-gradient-to-r from-blue-300 to-purple-800 text-transparent text-2xl font-sans font-bold text-center","children":"The Goal"}],["$","p",null,{"className":"mt-2 text-justify","children":"This experiment has aim to reduce the dimension of an image by using convolutional autoencoder approach."}]]}],["$","div",null,{"className":"max-w-200","children":[["$","h2",null,{"className":"bg-clip-text bg-gradient-to-r from-blue-300 to-purple-800 text-transparent text-2xl font-sans font-bold text-center","children":"Preprocessing"}],["$","p",null,{"className":"mt-2 text-justify","children":"This experiment used crop image dataset which contains several variation of crops, those are jute, maize, rice, sugarcane, and wheat. I loaded these images and converted them into tensor with the shape of 224 x 224 x 3 (RGB)."}]]}],["$","div",null,{"className":"max-w-200","children":[["$","h2",null,{"className":"bg-clip-text bg-gradient-to-r from-blue-300 to-purple-800 text-transparent text-2xl font-sans font-bold text-center","children":"Modeling"}],["$","p",null,{"className":"mt-2 text-justify ","children":"In this section, I used custom convolutional layer for encoding part and convolutional transpose for decoding part. The encoder part consist of 2 conv2d with 64 and 256 output channel respectively. Both of them has kernel size = 3, stride = 2, and padding = 1. On the other part, decoder consists of 2 ConvTranspose2d with 256 and 64 output channels respectively. Each of them also has kernel size = 4, stride = 2, and padding = 1. The detail of the whole architecture can be shown below."}],["$","div",null,{"className":"place-items-center","children":["$","$L4",null,{"src":"./images/convolutional_autoencoder/image_1.png","alt":"Full Convolutional AutoEncoder Network","width":600,"height":80,"className":"mt-5"}]}],["$","p",null,{"className":"mt-5 text-justify","children":"After defining the model architecture, then I trained my model for 50 epochs. To evaluate the performance of this model, I used Mean Absolute Error fro calculating reconstruction loss. Basically, the approach is pretty simple, I just calculated the difference between prediction values and original image pixels. It can be happened because the output of decoder will be a vector which has the same dimension as the input image shape. After 50 epochs, the reconstruction loss value decreased significantly as shown on the image below."}],["$","div",null,{"className":"place-items-center","children":["$","$L4",null,{"src":"./images/convolutional_autoencoder/image_2.png","alt":"Reconstruction Loss","width":500,"height":80,"className":"mt-5"}]}]]}],["$","div",null,{"className":"max-w-200","children":[["$","h2",null,{"className":"bg-clip-text bg-gradient-to-r from-blue-300 to-purple-800 text-transparent text-2xl font-sans font-bold text-center","children":"Inference"}],["$","p",null,{"className":"mt-2 text-justify","children":"In the inference process, I tested my trained model with testing dataset. The goal is to reduce the dimension of the input image using encoder part, then re-construct the input image using decoder part. The inference result can be defined to be successful if the output of encoder is very similar with the input image. I tested this network with 3 different images as shown below"}],["$","div",null,{"className":"flex flex-row flex-wrap gap-5 mt-5 justify-center lg-1000px:justify-start max-w-200","children":[["$","$L4",null,{"src":"./images/convolutional_autoencoder/image_3.png","alt":"Inference image result 1","width":500,"height":80}],["$","$L4",null,{"src":"./images/convolutional_autoencoder/image_4.png","alt":"Inference image result 2","width":500,"height":80}],["$","$L4",null,{"src":"./images/convolutional_autoencoder/image_5.png","alt":"Inference image result 3","width":500,"height":80}]]}],["$","p",null,{"className":"mt-5 text-justify","children":"As we can see on the above examples, the output images from decoder part are very similar with the input images. This means that our encoder part was successfully extract relevant information from the input image with low reconstruction loss. We can then use this network to reduce the dimension of our image using encoder part which is very useful to either storing image information as a vector or etc."}]]}]]}]]}],["$","$L5",null,{"children":"$L6"}],null,["$","$L7",null,{"children":["$L8","$L9",["$","$La",null,{"promise":"$@b"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Jrgk_6_PyT4cuHEL_k2VZ",{"children":[["$","$Lc",null,{"children":"$Ld"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$e","$undefined"],"s":false,"S":true}
f:"$Sreact.suspense"
10:I[4911,[],"AsyncMetadata"]
6:["$","$f",null,{"fallback":null,"children":["$","$L10",null,{"promise":"$@11"}]}]
9:null
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
8:null
11:{"metadata":[["$","title","0",{"children":"Create Next App"}],["$","meta","1",{"name":"description","content":"Generated by create next app"}],["$","link","2",{"rel":"icon","href":"/portfolio/favicon.ico","type":"image/x-icon","sizes":"16x16"}]],"error":null,"digest":"$undefined"}
b:{"metadata":"$11:metadata","error":null,"digest":"$undefined"}
